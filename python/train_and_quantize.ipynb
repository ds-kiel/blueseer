{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BlueSeer - Training, Quantization, and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io, time, copy\n",
    "\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import absl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle\n",
    "import seaborn as sn\n",
    "\n",
    "# Remove Tensorflow C-level logging and warnings\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Force font embedding when creating figure as PDF\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_path = \"../dataset/train\"\n",
    "test_set_path = \"../dataset/test\"\n",
    "output_path = \"./blueseer_model.cc\"\n",
    "\n",
    "base_path = \"./\"\n",
    "\n",
    "MODELS_DIR = os.path.join(base_path, 'models')\n",
    "if not os.path.exists(MODELS_DIR):\n",
    "    os.mkdir(MODELS_DIR)\n",
    "MODEL_TF = MODELS_DIR + 'model'\n",
    "MODEL_TFLITE = MODELS_DIR + 'model.tflite'\n",
    "MODEL_TFLITE_CPP = MODELS_DIR + 'blueseer_model.cc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Category Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine subclasses from two different data collection periods into one common category bbreakdown\n",
    "CLASS_MAPPING = {\n",
    "        'street':'street',\n",
    "        'park':'nature',\n",
    "        'apartment':'home', \n",
    "        'supermarket':'shopping', \n",
    "        'clothing_store':'shopping', \n",
    "        'train':'transport', \n",
    "        'bus':'transport', \n",
    "        'gym':'entertainment', \n",
    "        'car':'transport', \n",
    "        'house':'home', \n",
    "        'nature':'nature', \n",
    "        'restaurant':'restaurant', \n",
    "        'cinema':'entertainment', \n",
    "        'concert':'entertainment', \n",
    "        'plane':'transport',\n",
    "        'bar':'restaurant',\n",
    "        'shopping':'shopping',\n",
    "        'transport':'transport',\n",
    "        'home':'home',\n",
    "        'office':'office',\n",
    "        'mensa':'restaurant',\n",
    "        'lecture':'university', \n",
    "    }\n",
    "\n",
    "def original_classes_to_blueseer_classes(Y_labels):\n",
    "    mapping = CLASS_MAPPING.get\n",
    "    return [mapping(label, label) for label in Y_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing collected samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all paths to .CSV files\n",
    "def find_csv_filenames(path_to_dir, suffix=\".CSV\"):\n",
    "    filenames = []\n",
    "    for sub in os.walk(path_to_dir):\n",
    "        if sub[0] != path_to_dir:\n",
    "            # find all .CSV files within the folder\n",
    "            filenames += [sub[0] + \"/\" +\n",
    "            filename for filename in os.listdir(sub[0]) if filename.endswith(suffix)]\n",
    "    filenames.sort()\n",
    "    return filenames\n",
    "\n",
    "\n",
    "# Parse .CSV files into pandas Dataframes\n",
    "def parse_files_to_df(files):\n",
    "    dataframes = []\n",
    "    num_files = len(files)\n",
    "    i=0\n",
    "    last_displayed = -1\n",
    "    for f in files:\n",
    "        df = pd.read_csv(f)\n",
    "        dataframes.append(df)\n",
    "        i+=1\n",
    "        if int(i/num_files*100) != last_displayed:\n",
    "            print(f\"{int(i/num_files*100)}% loaded ({i}/{num_files})\")\n",
    "            last_displayed = int(i/num_files*100)\n",
    "    return dataframes\n",
    "\n",
    "\n",
    "\n",
    "#most common services in selected environments\n",
    "SERVICES = [\t\"0af0\", \"1802\", \"180f\", \"1812\", \"1826\", \"2222\", \"ec88\", \"fd5a\",\n",
    "    \"fd6f\", \"fdd2\", \"fddf\", \"fe03\", \"fe07\", \"fe0f\", \"fe61\", \"fe9f\",\n",
    "    \"fea0\", \"feb9\", \"febe\", \"fee0\", \"ff0d\", \"ffc0\", \"ffe0\"]\n",
    "\n",
    "# features collected that are not used when creating the dataset\n",
    "MUST_REMOVE_COLUMNS = [\" services\", \" manufacturer_data_lengths\",' time_point_1', ' time_point_2', ' time_point_3']\n",
    "\n",
    "def process_files(dataframes,\n",
    "                  without_services = False,\n",
    "                  only_labels = None,\n",
    "                  remove_columns = None,\n",
    "                  verbose=False,\n",
    "                 ):\n",
    "\n",
    "    X_datapoints = []\n",
    "    Y_labels = []\n",
    "    available_columns = dataframes[0].columns.tolist()\n",
    "    # remove label from columns\n",
    "    available_columns.remove(\"label\")\n",
    "    # remove deprecated columns that might exist in the dataset\n",
    "    for col in MUST_REMOVE_COLUMNS:\n",
    "        try:\n",
    "            available_columns.remove(col) # there must be a leading space!\n",
    "        except Exception as e:\n",
    "            #print(f\"{col} not found in dataset\")\n",
    "            pass\n",
    "    # remove any column we wish to get rid of\n",
    "    if remove_columns is not None:\n",
    "        for col in remove_columns:\n",
    "            try:\n",
    "                available_columns.remove(col) # there must be a leading space!\n",
    "            except Exception as e:\n",
    "                #print(f\"{col} not found in dataset\")\n",
    "                pass\n",
    "    # if we want to get rid of the 23most-common services, do so now\n",
    "    if without_services:\n",
    "        for serv in SERVICES:\n",
    "            available_columns.remove(f\" {serv}\") # there is always a leading space!\n",
    "\n",
    "    # for-each dataframe, add it to the dataset\n",
    "    for df in dataframes:\n",
    "        # Find label from first row\n",
    "        label = df.iloc[0][\"label\"]\n",
    "        X_datapoints.append(df[available_columns].to_numpy().flatten()) # .iloc[:num_scans-1]\n",
    "        Y_labels.append(label)\n",
    "    \n",
    "    X_datapoints = np.array(X_datapoints)\n",
    "    \n",
    "    # Update classes to DAC classes\n",
    "    Y_labels = original_classes_to_blueseer_classes(Y_labels)\n",
    "    Y_labels = np.array(Y_labels)\n",
    "    \n",
    "    # Remove entertainment class & samples from dataset\n",
    "    data_to_keep = Y_labels!='entertainment'#np.array([y!='entertainment' for y in Y_labels])\n",
    "    X_datapoints = X_datapoints[data_to_keep]\n",
    "    Y_labels = Y_labels[data_to_keep]\n",
    "    Y_labels = list(Y_labels)\n",
    "    Y_labels = np.array(Y_labels)\n",
    "    # university\n",
    "    data_to_keep = Y_labels!='university'#np.array([y!='entertainment' for y in Y_labels])\n",
    "    X_datapoints = X_datapoints[data_to_keep]\n",
    "    Y_labels = Y_labels[data_to_keep]\n",
    "    Y_labels = list(Y_labels)\n",
    "\n",
    "    # Check how many samples per environment where found\n",
    "    unique_labels = set(Y_labels)\n",
    "    if verbose:\n",
    "        print(unique_labels)\n",
    "        for lbl in unique_labels:\n",
    "            print(f\"{lbl}: {Y_labels.count(lbl)}\")\n",
    "    unique_labels = list(unique_labels)\n",
    "    \n",
    "    return X_datapoints, Y_labels, unique_labels, available_columns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score normalization: find Mean and Standard deviation of the distribution\n",
    "def get_normalization_params(X):\n",
    "    X = np.array(X)\n",
    "    # Extract mean and std, per-feature\n",
    "    feature_mean = np.nanmean(X,axis=0)\n",
    "    feature_std = np.nanstd(X,axis=0)\n",
    "    return feature_mean, feature_std\n",
    "\n",
    "# X_normalized = (X - mean(X)) / STD(X)\n",
    "def normalize_data(X,\n",
    "                   Y_str,\n",
    "                   feature_mean,\n",
    "                   feature_std,\n",
    "                   labels,\n",
    "                   ):\n",
    "    X = np.array(X)\n",
    "    X = (X-feature_mean)/(feature_std+np.finfo(float).eps)\n",
    "    # Transform Y data from string to integer\n",
    "    Y = np.zeros((len(Y_str),))\n",
    "    for i in range(len(Y_str)):\n",
    "        Y[i] = labels.index(Y_str[i])\n",
    "    Y = np.array(Y,dtype=np.int8)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate C++ friendly representation of the normalization paramaters\n",
    "# The function is called when generation the C++ code of BlueSeer\n",
    "def generate_normalization_parameters_CPP(mean_list, std_list, labels):\n",
    "    mean_str = \"const float mean_list[] = {\"\n",
    "    for i in range(0, len(mean_list)):\n",
    "        if i != 0:\n",
    "            mean_str += \", \"\n",
    "        mean_str += str(mean_list[i])\n",
    "    mean_str += \"};\"\n",
    "    \n",
    "    std_str = \"const float std_list[] = {\"\n",
    "    for i in range(0, len(std_list)):\n",
    "        if i != 0:\n",
    "            std_str += \", \"\n",
    "        std_str += str(std_list[i])\n",
    "    std_str += \"};\"\n",
    "\n",
    "    labels_str = \"const char available_env[][16] = {\"\n",
    "    for i in range(0, len(labels)):\n",
    "        if i != 0:\n",
    "            labels_str += \", \"\n",
    "        labels_str += \"\\\"\"+labels[i]+\"\\\"\"\n",
    "    labels_str += \"};\"\n",
    "    return mean_str, std_str, labels_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Equalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalize_class_distribution(X, Y, remove_class=[]):\n",
    "    # Separate each class instance into bins\n",
    "    classes = list(np.unique(Y))\n",
    "    try:\n",
    "        classes.remove(remove_class)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    per_class_X = dict()\n",
    "    smallest_class = 999999\n",
    "    for cls in classes:\n",
    "        per_class_X[cls] = X[Y==cls]\n",
    "        smallest_class = per_class_X[cls].shape[0] if per_class_X[cls].shape[0] < smallest_class else smallest_class\n",
    "    # Shuffle data, keep an equal number of instances per class\n",
    "    for cls in classes:\n",
    "        np.random.shuffle(per_class_X[cls])\n",
    "        per_class_X[cls] = per_class_X[cls][:smallest_class]\n",
    "    # recombine into one array\n",
    "    X_new = np.concatenate([per_class_X[cls] for cls in classes], axis=0)\n",
    "    Y_new = np.array([cls for cls in classes for i in range(smallest_class)])\n",
    "    return X_new, Y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(training_path,\n",
    "                     test_path,\n",
    "                     without_services=True,\n",
    "                     verbose=False,\n",
    "                    ):\n",
    "    if verbose:\n",
    "        print(\"Loading the training dataset. This can take a few minutes.\")\n",
    "    train_dataframes = parse_files_to_df(find_csv_filenames(training_path))\n",
    "    if verbose:\n",
    "        print(\"Loading test dataset.\")\n",
    "    test_dataframes = parse_files_to_df(find_csv_filenames(test_path))\n",
    "    if verbose:\n",
    "        print(f\"Training set: {len(train_dataframes)} samples\")\n",
    "        print(f\"Test set: {len(test_dataframes)} samples\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Processing the training CSV files.\")\n",
    "    X_train, Y_train, labels, features = process_files(train_dataframes,\n",
    "                                                         without_services=without_services,\n",
    "                                                         verbose=verbose,\n",
    "                                                        )\n",
    "    # get per-feature normalization (mean and std)\n",
    "    feature_mean, feature_std = get_normalization_params(X=X_train)\n",
    "    # normalize dataset\n",
    "    X_train, Y_train = normalize_data(X_train,\n",
    "                                          Y_train,\n",
    "                                          feature_mean,\n",
    "                                          feature_std,\n",
    "                                          labels)\n",
    "    if verbose:\n",
    "        print(\"Processing the test CSV files.\")\n",
    "    X_test, Y_test, _, _ = process_files(test_dataframes,\n",
    "                                         only_labels = labels,\n",
    "                                         without_services=without_services,\n",
    "                                         verbose=verbose,\n",
    "                                        )\n",
    "    X_test, Y_test = normalize_data(X_test,\n",
    "                                        Y_test,\n",
    "                                        feature_mean,\n",
    "                                        feature_std,\n",
    "                                        labels\n",
    "                                       )\n",
    "    \n",
    "    # equalizing eval dataset\n",
    "    print(\"equalizing test set.\")\n",
    "    X_test,Y_test = equalize_class_distribution(X_test, Y_test)\n",
    "    \n",
    "    prepared = dict()\n",
    "    prepared['training'] = (X_train, Y_train)\n",
    "    prepared['test'] = (X_test, Y_test)\n",
    "    prepared['feature_mean'] = feature_mean\n",
    "    prepared['feature_std'] = feature_std\n",
    "    prepared['labels'] = labels\n",
    "    prepared['features'] = features\n",
    "    \n",
    "    try:\n",
    "        f =  open('prepared_dataset.pickle', 'wb')\n",
    "        pickle.dump(prepared, f)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    return prepared\n",
    "\n",
    "def load_prepared_datasets(filename='prepared_dataset.pickle'):\n",
    "    f =  open(filename, 'rb')\n",
    "    prepared = pickle.load(f)\n",
    "    return prepared\n",
    "\n",
    "# Divide training dataset into train and test sets\n",
    "def split_training_set(X_train, Y_train):\n",
    "    return X_train, Y_train, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. BlueSeer Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BlueSeer_model(num_classes):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(units=500,activation=\"relu\",kernel_regularizer=tf.keras.regularizers.l2(1e-5),name=\"layer0_dense\"))\n",
    "    model.add(tf.keras.layers.Dropout(0.5,name=\"layer0_dropout\"))\n",
    "    model.add(tf.keras.layers.Dense(units=num_classes,activation=\"softmax\",kernel_regularizer=tf.keras.regularizers.l2(1e-5),name=\"layer2_class_output\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "# learning Rate decay\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch < 4:\n",
    "        return 0.01\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "\n",
    "def get_smallest_model(num_classes, input_shape):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(units=num_classes,input_shape=input_shape,activation=\"softmax\",kernel_regularizer=tf.keras.regularizers.l2(1e-5),name=\"layer2_class_output\"))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(history):\n",
    "      # plot loss during training\n",
    "      plt.subplot(211)\n",
    "      plt.title('Loss')\n",
    "      plt.plot(history.history['loss'], label='train')\n",
    "      plt.plot(history.history['val_loss'], label='test')\n",
    "      plt.legend()\n",
    "      # plot accuracy during training\n",
    "      plt.subplot(212)\n",
    "      plt.title('Accuracy')\n",
    "      plt.plot(history.history['sparse_categorical_accuracy'], label='train')\n",
    "      plt.plot(history.history['val_sparse_categorical_accuracy'], label='test')\n",
    "      plt.legend()\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train,\n",
    "                Y_train,\n",
    "                X_val,\n",
    "                Y_val,\n",
    "                labels,\n",
    "                epochs=20,\n",
    "                batch_size=32,\n",
    "                verbose=0,\n",
    "               ):\n",
    "    \n",
    "    model = get_BlueSeer_model(len(labels))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.01,\n",
    "                                        decay=1e-6,\n",
    "                                        momentum=0.9)\n",
    "    # Exponentially Decreasing Learning Rate Decay\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics='sparse_categorical_accuracy'\n",
    "                 )\n",
    "    \n",
    "    model.build(input_shape=(None,115))\n",
    "    model.summary()\n",
    "\n",
    "    # Step 1: Training\n",
    "    history = model.fit(X_train,\n",
    "                        Y_train,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(X_val, Y_val),\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[lr_callback],\n",
    "                        verbose=verbose,\n",
    "                       )\n",
    "    if verbose:\n",
    "        plot_results(history)\n",
    "    \n",
    "    # Step 2: Quantization-Aware Training (fine-tuning)\n",
    "    model = tfmot.quantization.keras.quantize_model(model)\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.01,\n",
    "                                        decay=1e-6,\n",
    "                                        momentum=0.9)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics='sparse_categorical_accuracy'\n",
    "                 )\n",
    "    history = model.fit(X_train,\n",
    "                          Y_train,\n",
    "                          epochs=epochs//2,\n",
    "                          validation_data=(X_val, Y_val),\n",
    "                          batch_size=batch_size,\n",
    "                          callbacks=[lr_callback],\n",
    "                          verbose=verbose,\n",
    "                          )\n",
    "    \n",
    "    return model, history.history['val_sparse_categorical_accuracy'][-1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_dict = None\n",
    "try:\n",
    "    prepared_dict = load_prepared_datasets()\n",
    "except Exception as e:\n",
    "    prepared_dict = prepare_datasets(train_set_path, test_set_path, without_services=True, verbose=True)\n",
    "\n",
    "(X_train_original,Y_train_original) = prepared_dict['training']\n",
    "(X_test, Y_test) = prepared_dict['test']\n",
    "labels = prepared_dict['labels']\n",
    "feature_mean = prepared_dict['feature_mean']\n",
    "feature_std = prepared_dict['feature_std']\n",
    "feature_std = np.maximum(feature_std, 1.0)\n",
    "features = prepared_dict['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = None\n",
    "best_acc = 0.0\n",
    "for i in range(10):\n",
    "    # Create test split\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_train_original, Y_train_original, test_size=0.1)\n",
    "    # Create and eval model\n",
    "    model, val_acc = train_model(X_train,Y_train,\n",
    "                                  X_val,Y_val,\n",
    "                                  labels,\n",
    "                                  verbose=1,\n",
    "                                 )\n",
    "    loss, test_acc = model.evaluate(X_test, Y_test)\n",
    "    if test_acc > best_acc:\n",
    "        best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, test_acc = best_model.evaluate(X_test, Y_test)\n",
    "print(f\"Model Accuracy on the unseen test data: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(Y_test, np.argmax(best_model.predict(X_test),axis=1))\n",
    "print(cm)\n",
    "max_v = np.sum(cm[0])\n",
    "cm = pd.DataFrame(cm, index = [lbl.capitalize() for lbl in labels],\n",
    "                  columns = [lbl.capitalize() for lbl in labels])\n",
    "\n",
    "plt.figure(figsize = (4,4))\n",
    "ax = sn.heatmap(cm/max_v*100,\n",
    "           annot=True,\n",
    "           fmt='.1f',\n",
    "           cmap=\"Blues\",\n",
    "           cbar=False,\n",
    "              )\n",
    "ax.set_ylabel(\"True Class\", fontdict= {'fontweight':'bold'})\n",
    "ax.set_xlabel(\"Predicted Class\", fontdict= {'fontweight':'bold'})\n",
    "\n",
    "# plt.tight_layout()\n",
    "# matplotlib.rcParams.update({'font.size': 10})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Conversion & Export to C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_model(original_model, X_train):\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(original_model)\n",
    "    # Set the optimization flag\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    # Enforce integer only quantization\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "    # We keep float for input and output\n",
    "    converter.inference_input_type = tf.float32\n",
    "    converter.inference_output_type = tf.float32\n",
    "    # Provide a representative dataset to ensure we quantize correctly\n",
    "    def representative_dataset():\n",
    "        for i in range(len(X_train)):\n",
    "            yield([np.float32(X_train[i]).reshape(1, len(X_train[0]))])\n",
    "    converter.representative_dataset = representative_dataset\n",
    "    # Convert model\n",
    "    embedded_model = converter.convert()\n",
    "    # Temporary save the model to measure its size\n",
    "    open(\"temp_model\", \"wb\").write(embedded_model)\n",
    "    size = os.path.getsize(\"temp_model\")\n",
    "    \n",
    "    return embedded_model, size\n",
    "\n",
    "def export_model(model_tflite,\n",
    "                 target_filename,\n",
    "                 feature_mean,\n",
    "                 feature_std,\n",
    "                 labels,\n",
    "                 X_train,\n",
    "                 verbose = True):\n",
    "    \n",
    "    # Get normalization as string, to transform into C-compliant file\n",
    "    mean_str, std_str,labels_str = generate_normalization_parameters_CPP(mean_list=feature_mean,\n",
    "                                                         std_list= feature_std,\n",
    "                                                         labels=labels)\n",
    "    tflite_filename = f\"{target_filename}_tflite\"\n",
    "    tflm_filename = f\"{target_filename}_tflm\"\n",
    "    open(tflite_filename, \"wb\").write(model_tflite)\n",
    "\n",
    "    # Convert to a C source file, i.e, a TensorFlow Lite for Microcontrollers model\n",
    "    !xxd -i {tflite_filename} > {tflm_filename}\n",
    "    tflite_filename = tflite_filename.replace('/', '_').replace('.', '_')\n",
    "    !sed -i '' -e 's/'{tflite_filename}'/g_model/g' {tflm_filename}\n",
    "    \n",
    "    !rm -f ./constants.cc\n",
    "    model_str = \"alignas(16) const unsigned char bluesser_model[] = \"\n",
    "    with open(tflm_filename, 'r') as file:\n",
    "        data = file.read();\n",
    "        model_str += data[data.index(\"{\"): len(data)].replace(\"unsigned\", \"const\")\n",
    "\n",
    "    output_str = \"\"\n",
    "    output_str += \"#include \\\"constants.h\\\"\\n\"\n",
    "    output_str += mean_str +\"\\n\"\n",
    "    output_str += std_str + \"\\n\"\n",
    "    output_str += labels_str + \"\\n\"\n",
    "    output_str += \"const int available_env_len = \"+str(len(labels)) +\";\\n\"\n",
    "    output_str += model_str\n",
    "\n",
    "    with open(target_filename, \"w\") as file:\n",
    "        file.write(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tflite, size = convert_model(best_model, X_train)\n",
    "export_model(model_tflite,\"models/blueseer_model.cc\",feature_mean,feature_std,labels,X_train)\n",
    "print(f\"Model converted! Final Model Size: {size/1000} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ble_environment_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
